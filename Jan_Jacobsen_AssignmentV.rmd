---
title: "DSPM Assignment 05"
author: "Jan Jacobsen"
date: "1/11/2022"
output: 
  html_document:
    toc: true
---


```{r setup}
# set default options for code chunks
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

# set working directory
my_path <- r"{E:\R\DSPM_Assignment_05\}"
knitr::opts_knit$set(root.dir = my_path)

# clear workspace
rm(list = ls())


# include api storage file 
source("api_key_storage.r") 

# if you are not the creator: set your own API key instead using the following line
# api_key <- "your key"
```


```{r}
# imports
if (!require("tidyverse")) install.packages("tidyverse")
if (!require("jsonlite")) install.packages("jsonlite")
if (!require("httr")) install.packages("httr")
if (!require("ggplot2")) install.packages("ggplot2")
if (!require("maps")) install.packages("maps")


library(tidyverse)
library(jsonlite)
library(httr)
library(ggplot2)
library(maps)
```



```{r api_basics}
# retrieve german venue data (without specifiying page)
response_1_raw <- 
  GET(
    url = "https://app.ticketmaster.com/discovery/v2/venues?",
    query = list(countryCode = "DE",
              locale = "*",
              apikey = api_key)
  )
       
# get R list from response content json
response_1 <- jsonlite::fromJSON(content(response_1_raw, as = "text"))

# extract venue data from response
df <- response_1[["_embedded"]][["venues"]]

# build dataframe with required variables from retrieved data
venue_df_1 <- data.frame(
                name = df$name,
                city = df$city$name, 
                postalCode = df$postalCode,
                address = df$address$line1,
                url = df$url,
                long = as.double(df$location$longitude),
                lat = as.double(df$location$latitude),
                stringsAsFactors = FALSE
                )

glimpse(venue_df_1)
```

```{r api_advanced}
#get number of elements (to check later)
total_elements <- response_1$page$totalElements

# get number of pages to loop through (should be 629)
total_pages <- response_1$page$totalPages

# pages start at zero
start_page <- 0

# define dataframe to contain the entire dataset (same structure as in previous task)
df_venue_data_2 <- 
  data.frame(
    name = character(),
    city = character(),
    postalCode = character(),
    address = character(),
    url = character(),
    long = double(),
    lat = double(),
    stringsAsFactors = FALSE
)

# loop through pages
for (p in start_page:(total_pages-1)){
  
# retrieve german venue data (without specifiying page)
response_2_raw <- 
  GET(
    url = "https://app.ticketmaster.com/discovery/v2/venues?",
    query = list(countryCode = "DE",
              locale = "*",
              page = as.character(p), # current page
              apikey = api_key)
  )
       
# get R list from response content json
response_2 <- jsonlite::fromJSON(content(response_2_raw, as = "text"))

# extract venue data from response
df <- response_2[["_embedded"]][["venues"]]

# build (interim) df with required variables from retrieved data
# note: if every single row in the current page is missing a certain variable, 
#       the response object does not include this column. 
#       ifelse() clauses are used to handle these cases.
df_single_page <- data.frame(
                    name = (if ("name" %in% names(df)) {df$name} else {NA}),
                    city = (if ("city" %in% names(df)) {df$city$name} else {NA}),
                    postalCode = (if ("postalCode" %in% names(df)) {df$postalCode} else {NA}),
                    address = (if ("address" %in% names(df)) {df$address$line1} else {NA}),
                    url = (if ("url" %in% names(df)) {df$url} else {NA}),
                    long = (if ("location" %in% names(df)) {as.double(df$location$longitude)} else {NA}),
                    lat = (if ("location" %in% names(df)) {as.double(df$location$latitude)} else {NA}),
                    stringsAsFactors = FALSE
                  )

# append to main df
df_venue_data_2 <- df_venue_data_2 %>% bind_rows(df_single_page)

# delay to comply with API restrictions 
# minimum 0.2 seconds between requests is required according to ticketmaster, 
# but values slightly above this limit seem to get throttled anyways before all 
# pages are finished. The exact restriction policy is unclear.)
Sys.sleep(0.5) 

# print progress to console
print(paste0("page ", as.character(p), " data retrieved"))
}


# check if all observations were retrieved
print(paste0("Total available elements according to response metadata: ", as.character(total_elements)))
print(paste0("Rows in retrieved dataframe: ", as.character(nrow(df_venue_data_2))))

glimpse(df_venue_data_2)

```

```{r}
# define country name (for ggplot "world" map data set)
country_name = "Germany"

# define longitude and latitude boundaries for the country
min_long <- 5.866944
max_long <- 15.043611
min_lat <- 47.271679
max_lat <- 55.0846

# visualization (code given in assignment + adjustments)
ggplot() +
  geom_polygon(
    aes(x = long, y = lat, group = group), 
    data = map_data("world", region = country_name),
    fill = "grey90",color = "black") +
  theme_void() + 
  coord_quickmap() +
  labs(title = paste0("Event locations across ", country_name), caption = "Source: ticketmaster.com") +
  theme(title = element_text(size=8, face='bold'),
    plot.caption = element_text(face = "italic"))
```

