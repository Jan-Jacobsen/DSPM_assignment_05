---
title: "DSPM Assignment V: GitHub and the ticketmaster.com API"
author: "Jan Jacobsen"
date: "1/11/2022"
output: 
  html_document:
    toc: true
---

I worked together with Marvin Hoberg on parts of this assignment. I hereby assure that my submission is in line with the code of conduct outlined on the lecture slides.

The public GitHub repository for this project can be accessed here:
https://github.com/Jan-Jacobsen/DSPM_assignment_05


# Setup

Chunk settings, working directory, API key and imports

Note: The ticketmaster API key is stored in a separate file (in the variable api_key) to avoid making it publicly visible on GitHub.

```{r setup}
# set default options for code chunks
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

# set working directory
my_path <- r"{E:\R\DSPM_Assignment_05\}"
knitr::opts_knit$set(root.dir = my_path)

# clear workspace
rm(list = ls())

# include api storage file 
source("api_key_storage.r") 

# if you are not the creator: comment out the previous line and set your own API key below instead:
# api_key <- "your key"
```


```{r imports}
# imports
if (!require("tidyverse")) install.packages("tidyverse")
if (!require("jsonlite")) install.packages("jsonlite")
if (!require("httr")) install.packages("httr")
if (!require("ggplot2")) install.packages("ggplot2")
if (!require("maps")) install.packages("maps")

library(tidyverse)
library(jsonlite)
library(httr)
library(ggplot2)
library(maps)
```

# 1. API basics

The parameter-based ticketmaster API can be queried using ``GET()`` from the {httr}-package. I am using the {jsonlite}-package to parse the response object into an R list and save the relevant venue data in a new dataframe ``venue_data``.

This first request using the url for venue search and ``"DE"`` as the country parameter returns a single "page", which only includes 20 observations at a time.

```{r api_basics}
# retrieve german venue data (without specifiying page)
response_raw <- 
  GET(
    url = "https://app.ticketmaster.com/discovery/v2/venues?",
    query = list(countryCode = "DE",
              locale = "*",
              apikey = api_key)
  )
       
# get R list from response content json
response <- jsonlite::fromJSON(content(response_raw, as = "text"))

# extract venue data from response
df <- response[["_embedded"]][["venues"]]

# build dataframe with required variables from retrieved data
venue_data <- data.frame(
                name = df$name,
                city = df$city$name, 
                postalCode = df$postalCode,
                address = df$address$line1,
                url = df$url,
                long = as.double(df$location$longitude),
                lat = as.double(df$location$latitude),
                stringsAsFactors = FALSE
                )

glimpse(venue_data)
```


# 2. API advanced - German venue data

In order to retrieve the entire venue data set for a given country, every single page needs to be queried individually. The response object retrieved from an API query includes information about the total number of result pages for this query, which I use to define the query loop. 

The data frame ``venue_data_1`` is appended repeatedly with the data of a new page until the full data set is retrieved. An issue with the API is that the structure of the response object can change based on the results. Specifically, if all 20 observations of the page have missing values in the same column (e.g. sometimes the case for latitude and longitude), the response object does not seem to include the variable at all. This needs to be taken into account when extracting the relevant data from the API response. 

Ticketmaster states on their website that the free API key is valid for up to 5 requests per second. There do seem to be additional traffic surge restrictions though which can sometimes cause a "SpikeArrestViolation" error to be returned. To reliably avoid this, I set the interval between requests comfortably above the limit at 0.5s.

```{r api_advanced}
# define country code for ticketmaster API
country_code = "DE" 

#get number of elements (to check later)
total_elements <- response$page$totalElements

# get number of pages to loop through (should be 629)
total_pages <- response$page$totalPages

# pages start at zero
start_page <- 0

# define dataframe to contain the entire dataset (same structure as in previous task)
venue_data_1 <- 
  data.frame(
    name = character(),
    city = character(),
    postalCode = character(),
    address = character(),
    url = character(),
    long = double(),
    lat = double(),
    stringsAsFactors = FALSE
)

# loop through pages
for (p in start_page:(total_pages-1)){
    
  # retrieve venue data (for a single page)
  response_1_raw <- 
    GET(
      url = "https://app.ticketmaster.com/discovery/v2/venues?",
      query = list(countryCode = country_code,
                locale = "*",
                page = as.character(p), # current page
                apikey = api_key)
    )
         
  # get R list from response content json
  response_1 <- jsonlite::fromJSON(content(response_1_raw, as = "text"))
  
  # extract venue data from response 
  df <- response_1[["_embedded"]][["venues"]]
  
  # build (interim) df with required variables from retrieved data
  # note: if every single row in the current page is missing a certain variable, 
  #       the response object does not include this column. 
  #       ifelse() clauses are used to handle these cases.
  df_single_page <- data.frame(
                      name = (if ("name" %in% names(df)) {df$name} else {NA}),
                      city = (if ("city" %in% names(df)) {df$city$name} else {NA}),
                      postalCode = (if ("postalCode" %in% names(df)) {df$postalCode} else {NA}),
                      address = (if ("address" %in% names(df)) {df$address$line1} else {NA}),
                      url = (if ("url" %in% names(df)) {df$url} else {NA}),
                      long = (if ("location" %in% names(df)) {as.double(df$location$longitude)} else {NA}),
                      lat = (if ("location" %in% names(df)) {as.double(df$location$latitude)} else {NA}),
                      stringsAsFactors = FALSE
                    )
  
  # append to main df
  venue_data_1 <- venue_data_1 %>% bind_rows(df_single_page)
  
  # delay to comply with API restrictions 
  Sys.sleep(0.5) 
  
  # print progress to console
  # print(paste0("page ", as.character(p), " data retrieved"))
}

# check if all observations were retrieved
print(paste0("Total available elements according to response metadata: ", as.character(total_elements)))
print(paste0("Rows in retrieved dataframe: ", as.character(nrow(venue_data_1))))

glimpse(venue_data_1)

```

The visualization is produced with the suggested code from the assignment. Because some of the observations have faulty location data which would distort the plot, all coordinate data outside of the rectangle specified by Germany's extreme points is discarded.

```{r}
# define country name (for ggplot "world" map data set)
country_name = "Germany"

# define longitude and latitude boundaries for the country
min_long <- 5.866944
max_long <- 15.043611
min_lat <- 47.271679
max_lat <- 55.0846

# set coordinates outside the boundaries to NA
venue_data_1 <- 
  venue_data_1 %>%  # transform longitude column
    mutate(long = ifelse(long > max_long | long < min_long | is.na(lat) |
                  lat > max_lat | lat < min_lat, NA, long)
    ) %>% # transform latitude column
    mutate(lat = ifelse(lat > max_lat | lat < min_lat | is.na(long), NA, lat))

# visualization (adjusted code template from assignment)
ggplot() +
  # country map
  geom_polygon(aes(x = long, y = lat, group = group), 
    data = map_data("world", region = country_name),
    fill = "grey90",color = "black") +
  theme_void() + 
  # event location points
  geom_point(aes(x = long, y = lat),
    data = venue_data_1, color = "purple", size = 0.5) +
  # projection
  coord_quickmap() +
  # design & descriptions
  labs(title = paste0("Event locations across ", country_name), caption = "Source: ticketmaster.com") +
  theme(title = element_text(size=8, face='bold'),
    plot.caption = element_text(face = "italic"))
```

# 3. API advanced - Swiss venue data

The same code is now used again to retrieve and plot venue data for Switzerland. The only required adjustments are the following:

* Defining the country name for the ggplot2 "world" map data set (here: "Switzerland")
* Defining the latitude and longitude boundaries to eliminate faulty location data.
* Defining the country code used by the API (here: "CH")


```{r echo = FALSE}
# repeat data gathering process for second country: Switzerland

# define country code for ticketmaster API
country_code = "CH" 

# retrieve single page first to get information about the total number of pages
response_2_raw <- 
  GET(
    url = "https://app.ticketmaster.com/discovery/v2/venues?",
    query = list(countryCode = country_code,
              locale = "*",
              apikey = api_key)
  )
       
# get R list from response content json
response_2 <- jsonlite::fromJSON(content(response_2_raw, as = "text"))

#get number of elements (to check later)
total_elements <- response_2$page$totalElements

# get number of pages to loop through (should be 629)
total_pages <- response_2$page$totalPages

# pages start at zero
start_page <- 0

# define dataframe to contain the entire dataset
venue_data_2 <- 
  data.frame(
    name = character(),
    city = character(),
    postalCode = character(),
    address = character(),
    url = character(),
    long = double(),
    lat = double(),
    stringsAsFactors = FALSE
)

# start full data retrieval

# loop through pages
for (p in start_page:(total_pages-1)){
    
  # retrieve german venue data (without specifiying page)
  response_2_raw <- 
    GET(
      url = "https://app.ticketmaster.com/discovery/v2/venues?",
      query = list(countryCode = country_code,
                locale = "*",
                page = as.character(p), # current page
                apikey = api_key)
    )
         
  # get R list from response content json
  response_2 <- jsonlite::fromJSON(content(response_2_raw, as = "text"))
  
  # extract venue data from response
  df <- response_2[["_embedded"]][["venues"]]
  
  # build (interim) df with required variables from retrieved data
  df_single_page <- data.frame(
                      name = (if ("name" %in% names(df)) {df$name} else {NA}),
                      city = (if ("city" %in% names(df)) {df$city$name} else {NA}),
                      postalCode = (if ("postalCode" %in% names(df)) {df$postalCode} else {NA}),
                      address = (if ("address" %in% names(df)) {df$address$line1} else {NA}),
                      url = (if ("url" %in% names(df)) {df$url} else {NA}),
                      long = (if ("location" %in% names(df)) {as.double(df$location$longitude)} else {NA}),
                      lat = (if ("location" %in% names(df)) {as.double(df$location$latitude)} else {NA}),
                      stringsAsFactors = FALSE
                    )
  
  # append to main df
  venue_data_2 <- venue_data_2 %>% bind_rows(df_single_page)
  
  # delay to comply with API restrictions 
  Sys.sleep(0.5) 
  
  # print progress to console
  # print(paste0("page ", as.character(p), " data retrieved"))
  
}

# check if all observations were retrieved
print(paste0("Total available elements according to response metadata: ", as.character(total_elements)))
print(paste0("Rows in retrieved dataframe: ", as.character(nrow(venue_data_2))))

glimpse(venue_data_2)
```

```{r echo = FALSE}
# Data cleaning & visualization for second country: Switzerland

# define country name (for ggplot "world" map data set)
country_name = "Switzerland"

# define longitude and latitude boundaries for the country
# (source: https://en.wikipedia.org/wiki/List_of_extreme_points_of_Switzerland)
min_long <- 5.956303
max_long <- 10.491944
min_lat <- 45.818031
max_lat <- 47.808264

# set coordinates outside the boundaries to NA
venue_data_2 <- 
  venue_data_2 %>%  # transform longitude column
    mutate(long = ifelse(long > max_long | long < min_long | is.na(lat) |
                  lat > max_lat | lat < min_lat, NA, long)
    ) %>% # transform latitude column
    mutate(lat = ifelse(lat > max_lat | lat < min_lat | is.na(long), NA, lat))

# visualization (adjusted code template from assignment)
ggplot() +
  # country map
  geom_polygon(aes(x = long, y = lat, group = group), 
    data = map_data("world", region = country_name),
    fill = "grey90",color = "black") +
  theme_void() + 
  # event location points
  geom_point(aes(x = long, y = lat),
    data = venue_data_2, color = "blue", size = 1) +
  # projection
  coord_quickmap() +
  # design & descriptions
  labs(title = paste0("Event locations across ", country_name), caption = "Source: ticketmaster.com") +
  theme(title = element_text(size=8, face='bold'),
    plot.caption = element_text(face = "italic"))
```



# 4. Note on data quality

The retrieved venue data sets present several issues with respect to data quality, some of which are the following:

* Location data is missing for many of the observations
* Some observations have obviously false location data (i.e. coordinates outside of country borders).
* Some observations have nonsensical values in other columns (i.e. name, city, postalCode, ...)
* Duplicate observations exist

Except for erasing coordinate values outside of the rectangle spanned by a country's extreme points, no additional measures were taken to achieve a cleaner data set. The main issue (missing location data) is not rectifiable without consulting other data sources. For our purposes (visual representation of the location data), duplicate observations and flawed values in columns other than the location data do not present major issues, which is why I chose to leave the data set as is.
